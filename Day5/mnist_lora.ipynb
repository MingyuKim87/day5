{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
       "           0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
       "           0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
       "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
       "           0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
       "           0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
       "           0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
       "           0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
       "           0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
       "           0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
       "           0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
       "           0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
       "           0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
       "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
       "           0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
       "           0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "odd_classes = [1, 3, 5, 7, 9]\n",
    "\n",
    "def filter_odd_indices(dataset):\n",
    "    indices = [i for i, (_, label) in enumerate(dataset) if label in odd_classes]\n",
    "    return indices\n",
    "\n",
    "train_indices = filter_odd_indices(train_dataset)\n",
    "test_indices = filter_odd_indices(test_dataset)\n",
    "\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "test_subset = Subset(test_dataset, test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader_iter = iter(train_loader)\n",
    "mini_batch = next(train_loader_iter)\n",
    "X, y = mini_batch[0], mini_batch[1]\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 9, 3, 7, 5, 7, 5, 3, 1, 9, 3, 3, 3, 7, 5, 3, 7, 1, 1, 3, 3, 3, 5, 7,\n",
       "        1, 5, 1, 1, 5, 9, 5, 1, 1, 1, 1, 9, 1, 1, 1, 3, 1, 3, 5, 5, 7, 1, 3, 7,\n",
       "        1, 3, 9, 1, 7, 7, 3, 9, 3, 7, 3, 1, 7, 7, 5, 7])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[0].permute(1,2,0))\n",
    "print(y[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_odd = MLP()\n",
    "\n",
    "# Forward test\n",
    "X = torch.rand(2, 1, 28, 28)\n",
    "y = model_odd(X)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_odd.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx*len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=-1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.0f}%)\\n')\n",
    "    return accuracy\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_odd.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/30508 (0%)]\tLoss: 2.293621\n",
      "Train Epoch: 1 [6400/30508 (21%)]\tLoss: 0.330007\n",
      "Train Epoch: 1 [12800/30508 (42%)]\tLoss: 0.275300\n",
      "Train Epoch: 1 [19200/30508 (63%)]\tLoss: 0.103199\n",
      "Train Epoch: 1 [25600/30508 (84%)]\tLoss: 0.070082\n",
      "\n",
      "Test set: Average loss: 0.0013, Accuracy: 29711/30508 (97%)\n",
      "\n",
      "------------------------------\n",
      "Train Epoch: 2 [0/30508 (0%)]\tLoss: 0.081466\n",
      "Train Epoch: 2 [6400/30508 (21%)]\tLoss: 0.010960\n",
      "Train Epoch: 2 [12800/30508 (42%)]\tLoss: 0.094521\n",
      "Train Epoch: 2 [19200/30508 (63%)]\tLoss: 0.143333\n",
      "Train Epoch: 2 [25600/30508 (84%)]\tLoss: 0.025703\n",
      "\n",
      "Test set: Average loss: 0.0009, Accuracy: 30044/30508 (98%)\n",
      "\n",
      "------------------------------\n",
      "Train Epoch: 3 [0/30508 (0%)]\tLoss: 0.027707\n",
      "Train Epoch: 3 [6400/30508 (21%)]\tLoss: 0.005694\n",
      "Train Epoch: 3 [12800/30508 (42%)]\tLoss: 0.014670\n",
      "Train Epoch: 3 [19200/30508 (63%)]\tLoss: 0.024490\n",
      "Train Epoch: 3 [25600/30508 (84%)]\tLoss: 0.023919\n",
      "\n",
      "Test set: Average loss: 0.0006, Accuracy: 30110/30508 (99%)\n",
      "\n",
      "------------------------------\n",
      "Train Epoch: 4 [0/30508 (0%)]\tLoss: 0.037654\n",
      "Train Epoch: 4 [6400/30508 (21%)]\tLoss: 0.029601\n",
      "Train Epoch: 4 [12800/30508 (42%)]\tLoss: 0.015495\n",
      "Train Epoch: 4 [19200/30508 (63%)]\tLoss: 0.026883\n",
      "Train Epoch: 4 [25600/30508 (84%)]\tLoss: 0.126235\n",
      "\n",
      "Test set: Average loss: 0.0005, Accuracy: 30216/30508 (99%)\n",
      "\n",
      "------------------------------\n",
      "Train Epoch: 5 [0/30508 (0%)]\tLoss: 0.001846\n",
      "Train Epoch: 5 [6400/30508 (21%)]\tLoss: 0.056365\n",
      "Train Epoch: 5 [12800/30508 (42%)]\tLoss: 0.009265\n",
      "Train Epoch: 5 [19200/30508 (63%)]\tLoss: 0.002067\n",
      "Train Epoch: 5 [25600/30508 (84%)]\tLoss: 0.017376\n",
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 30296/30508 (99%)\n",
      "\n",
      "------------------------------\n",
      "Train Epoch: 6 [0/30508 (0%)]\tLoss: 0.043547\n",
      "Train Epoch: 6 [6400/30508 (21%)]\tLoss: 0.043097\n",
      "Train Epoch: 6 [12800/30508 (42%)]\tLoss: 0.050891\n",
      "Train Epoch: 6 [19200/30508 (63%)]\tLoss: 0.003217\n",
      "Train Epoch: 6 [25600/30508 (84%)]\tLoss: 0.004153\n",
      "\n",
      "Test set: Average loss: 0.0004, Accuracy: 30246/30508 (99%)\n",
      "\n",
      "------------------------------\n",
      "Train Epoch: 7 [0/30508 (0%)]\tLoss: 0.026549\n",
      "Train Epoch: 7 [6400/30508 (21%)]\tLoss: 0.038554\n",
      "Train Epoch: 7 [12800/30508 (42%)]\tLoss: 0.007433\n",
      "Train Epoch: 7 [19200/30508 (63%)]\tLoss: 0.036934\n",
      "Train Epoch: 7 [25600/30508 (84%)]\tLoss: 0.009719\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 30418/30508 (100%)\n",
      "\n",
      "------------------------------\n",
      "Train Epoch: 8 [0/30508 (0%)]\tLoss: 0.000772\n",
      "Train Epoch: 8 [6400/30508 (21%)]\tLoss: 0.000970\n",
      "Train Epoch: 8 [12800/30508 (42%)]\tLoss: 0.000292\n",
      "Train Epoch: 8 [19200/30508 (63%)]\tLoss: 0.013692\n",
      "Train Epoch: 8 [25600/30508 (84%)]\tLoss: 0.000308\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 30433/30508 (100%)\n",
      "\n",
      "------------------------------\n",
      "Train Epoch: 9 [0/30508 (0%)]\tLoss: 0.001458\n",
      "Train Epoch: 9 [6400/30508 (21%)]\tLoss: 0.009388\n",
      "Train Epoch: 9 [12800/30508 (42%)]\tLoss: 0.001995\n",
      "Train Epoch: 9 [19200/30508 (63%)]\tLoss: 0.005785\n",
      "Train Epoch: 9 [25600/30508 (84%)]\tLoss: 0.005496\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 30413/30508 (100%)\n",
      "\n",
      "------------------------------\n",
      "Train Epoch: 10 [0/30508 (0%)]\tLoss: 0.000330\n",
      "Train Epoch: 10 [6400/30508 (21%)]\tLoss: 0.002568\n",
      "Train Epoch: 10 [12800/30508 (42%)]\tLoss: 0.010891\n",
      "Train Epoch: 10 [19200/30508 (63%)]\tLoss: 0.011074\n",
      "Train Epoch: 10 [25600/30508 (84%)]\tLoss: 0.000498\n",
      "\n",
      "Test set: Average loss: 0.0002, Accuracy: 30403/30508 (100%)\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model_odd, device, train_loader, optimizer, epoch)\n",
    "    accuracy = test(model_odd, device, test_loader)\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_odd.state_dict(), \"model_mnist_odd.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "train_loader_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    def __init__(self, in_features, out_features, r=4, alpha=None):\n",
    "        super().__init__()\n",
    "        self.r = r\n",
    "\n",
    "        if alpha is None:\n",
    "            self.alpha = r\n",
    "        else:\n",
    "            self.alpha = alpha\n",
    "\n",
    "        self.lora_A = nn.Linear(in_features, r, bias=False)\n",
    "        self.lora_B = nn.Linear(r, out_features, bias=False)\n",
    "\n",
    "        # B = 0\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "        # scaler\n",
    "        self.scale = self.alpha / r\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.lora_B(self.lora_A(x)) * self.scale\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPwithLoRA(nn.Module):\n",
    "    def __init__(self, lora_config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loca_config = lora_config\n",
    "\n",
    "        r = lora_config['r']\n",
    "        lora_alpha = lora_config['alpha']\n",
    "\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc1_lora = LoRA(28*28, 256, r=r, alpha=lora_alpha)\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc2_lora = LoRA(256, 128, r=r, alpha=lora_alpha)\n",
    "\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.fc1(x) + self.fc1_lora(x))\n",
    "        x = torch.relu(self.fc2(x) + self.fc2_lora(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def save_trainable_parameters(self, file_path):\n",
    "        trainable_state_dict= {\n",
    "            name:param for name, param in self.named_parameters() if param.requires_grad\n",
    "        }\n",
    "\n",
    "        trainable_state_dict['alpha'] = self.loca_config['alpha']\n",
    "        trainable_state_dict['r'] = self.loca_config['r']\n",
    "\n",
    "        torch.save(trainable_state_dict, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPwithLoRA(\n",
       "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (fc1_lora): LoRA(\n",
       "    (lora_A): Linear(in_features=784, out_features=4, bias=False)\n",
       "    (lora_B): Linear(in_features=4, out_features=256, bias=False)\n",
       "  )\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc2_lora): LoRA(\n",
       "    (lora_A): Linear(in_features=256, out_features=4, bias=False)\n",
       "    (lora_B): Linear(in_features=4, out_features=128, bias=False)\n",
       "  )\n",
       "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lora = MLPwithLoRA(lora_config={'r': 4, 'alpha': 8.0})\n",
    "model_lora.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "odd_state_dict = torch.load(\"model_mnist_odd.pth\", weights_only=True)\n",
    "model_lora.fc1.weight.data=odd_state_dict['fc1.weight']\n",
    "model_lora.fc1.bias.data=odd_state_dict['fc1.bias']\n",
    "model_lora.fc2.weight.data=odd_state_dict['fc2.weight']\n",
    "model_lora.fc2.bias.data=odd_state_dict['fc2.bias']\n",
    "model_lora.fc3.weight.data=odd_state_dict['fc3.weight']\n",
    "model_lora.fc3.bias.data=odd_state_dict['fc3.bias']\n",
    "\n",
    "for param in model_lora.fc1.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_lora.fc2.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_lora.fc3.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model_lora.fc1_lora.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_lora.fc2_lora.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(2, 1, 28, 28).to(device)\n",
    "y = model_lora(X)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 240842\n",
      "Trainable parameters: 5696\n",
      "Ratio of trainable parameters: 2.37%\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "total_params, trainable_params = count_parameters(model_lora)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(f\"Ratio of trainable parameters: {(trainable_params/total_params)*100:.3}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n",
      "10 4\n"
     ]
    }
   ],
   "source": [
    "numbers = [1,2,3,4,5,6,7,8,9,10]\n",
    "even_numbers = filter(lambda x: x % 2 == 0, numbers)\n",
    "print(list(even_numbers))\n",
    "\n",
    "print(\n",
    "    len(list(model_lora.parameters())), # 전체 5개의 Linear에서 w, b가 있으므로 10개\n",
    "    len(list(filter(lambda p: p.requires_grad, model_lora.parameters()))) # lora adapter 2개에서 A, B 있으므로 4개\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model_lora.parameters()),\n",
    "    lr=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 784])\n",
      "torch.Size([256, 4])\n",
      "torch.Size([4, 256])\n",
      "torch.Size([128, 4])\n"
     ]
    }
   ],
   "source": [
    "for params in optimizer.param_groups[0][\"params\"]:\n",
    "    print(params.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 11.830862\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.398032\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.040664\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.756486\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.647018\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.569160\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.369654\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.236448\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.298132\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.229623\n",
      "\n",
      "Test set: Average loss: 0.0048, Accuracy: 54799/60000 (91%)\n",
      "\n",
      "------------------------------\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.209692\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.158789\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.286688\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.144957\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.166203\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.369170\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.142362\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.362621\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.183464\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.175092\n",
      "\n",
      "Test set: Average loss: 0.0030, Accuracy: 56732/60000 (95%)\n",
      "\n",
      "------------------------------\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.242376\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.361221\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.138155\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.209779\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.306214\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.123983\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.074015\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.252581\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.124677\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.291640\n",
      "\n",
      "Test set: Average loss: 0.0024, Accuracy: 57224/60000 (95%)\n",
      "\n",
      "------------------------------\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.215898\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.204208\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.161106\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.087852\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.125159\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.074858\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.102807\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.055522\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.144429\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.184569\n",
      "\n",
      "Test set: Average loss: 0.0019, Accuracy: 57832/60000 (96%)\n",
      "\n",
      "------------------------------\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.177716\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.096207\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.171425\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.063864\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.118273\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.247878\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.146994\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.171602\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.122482\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.221908\n",
      "\n",
      "Test set: Average loss: 0.0017, Accuracy: 58083/60000 (97%)\n",
      "\n",
      "------------------------------\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.132429\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.047952\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.226102\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.043315\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.147970\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.118643\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.121024\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.070666\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.134808\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.097748\n",
      "\n",
      "Test set: Average loss: 0.0015, Accuracy: 58294/60000 (97%)\n",
      "\n",
      "------------------------------\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.021590\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.116612\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.032063\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.107887\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.048951\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.038398\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.024672\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.110179\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.051297\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.085687\n",
      "\n",
      "Test set: Average loss: 0.0014, Accuracy: 58398/60000 (97%)\n",
      "\n",
      "------------------------------\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.020007\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.157527\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.089035\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.030854\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.076900\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.070309\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.049165\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.142319\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs+1):\n",
    "    train(model_lora, device, train_loader, optimizer, epoch)\n",
    "    accuracy = test(model_lora, device, test_loader)\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1404, Accuracy: 30403/60000 (51%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0011, Accuracy: 58713/60000 (98%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "97.855"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model_odd, device, test_loader)\n",
    "test(model_lora, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 파라미터 저장\n",
    "model_lora.save_trainable_parameters(\"model_lora_trainable.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = MLP()\n",
    "\n",
    "odd_state_dict = torch.load(\"model_mnist_odd.pth\", weights_only=True)\n",
    "new_model.fc1.weight.data=odd_state_dict['fc1.weight']\n",
    "new_model.fc1.bias.data=odd_state_dict['fc1.bias']\n",
    "new_model.fc2.weight.data=odd_state_dict['fc2.weight']\n",
    "new_model.fc2.bias.data=odd_state_dict['fc2.bias']\n",
    "new_model.fc3.weight.data=odd_state_dict['fc3.weight']\n",
    "new_model.fc3.bias.data=odd_state_dict['fc3.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1404, Accuracy: 30403/60000 (51%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50.67166666666667"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(new_model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 8.0\n",
      "r: 4\n"
     ]
    }
   ],
   "source": [
    "lora_state_dict = torch.load(\"model_lora_trainable.pth\")\n",
    "for k in lora_state_dict.keys():\n",
    "    if \"weight\" in k:\n",
    "        print(f\"{k}: {lora_state_dict[k].shape}\")\n",
    "    else:\n",
    "        print(f\"{k}: {lora_state_dict[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lora_weights(model_base, lora_params):\n",
    "    a = lora_params['alpha']\n",
    "    r = lora_params['r']\n",
    "\n",
    "    lora_A_fc1 = lora_params['fc1_lora.lora_A.weight']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
